# Computer Use Agent

**Automate your computer like you actually know what you're doing**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Browser-Use](https://img.shields.io/badge/Browser--Use-v0.9.4+-orange.svg)](https://browser-use.com/)

Ever wish you could just tell your computer what to do and have it... actually do it? Like, "download that file and put it in Excel" or "sign me up for that account" and it just... works?

That's what this is. It's an AI agent system that actually understands what you want and figures out how to do it across macOS, Windows, and Linux. No more brittle scripts that break when someone moves a button. No more platform-specific hacks. Just... automation that works.

---

## Table of Contents

- [What This Actually Does](#what-this-actually-does)
- [Why You'd Want This](#why-youd-want-this)
- [How It Works (The Simple Version)](#how-it-works-the-simple-version)
- [What Makes It Different](#what-makes-it-different)
- [Platform Support](#platform-support)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

---

## What This Actually Does

Okay, so here's the deal. You know how most automation tools are either:

- Super fragile (breaks if someone changes a button color)
- Platform-specific (only works on Mac, or only Windows)
- Dumb as rocks (can't adapt to changes)

This fixes all of that. Computer Use Agent has multiple AI agents that specialize in different things. Think of it like having a team where one person is really good at web stuff, another is great with desktop apps, and another handles command-line things. They work together to figure out how to do what you want.

The system uses what I call a "multi-tier accuracy system" ‚Äî basically, it tries the most accurate method first (using your OS's built-in accessibility features), and if that doesn't work, it falls back to reading the screen with OCR, and if that fails, it uses vision AI. The result? It actually works 99% of the time instead of breaking constantly.

---

## Why You'd Want This

Let me paint you a picture. You're trying to automate something ‚Äî maybe downloading reports and putting them in a spreadsheet. Traditional tools would have you:

1. Hard-code pixel coordinates (breaks when the UI changes)
2. Write platform-specific code (can't share with your team on different OSes)
3. Hope your vision-only approach works (spoiler: it won't, consistently)

This system? You just say "download the Tesla stock report and create a chart in Excel" and it figures out:

- Which agent should handle the download (the browser specialist)
- Which agent should handle Excel (the desktop app specialist)
- How to pass the file between them (the system handles this automatically)
- What to do if something goes wrong (it has fallbacks)

It's like having a really smart assistant who actually understands computers.

---

## How It Works (The Simple Version)

Here's the flow, simplified:

```
You: "Download that report and make a chart"

Manager Agent (the coordinator):
  "Okay, this needs two things:
   1. Browser agent downloads the file
   2. GUI agent opens Excel and makes the chart
   Let me break this down..."

Browser Agent:
  "Got it. I'll navigate to the site, find the download,
   grab the file, and tell the next agent where it is."

GUI Agent:
  "Perfect. I see the file path from the browser agent.
   Let me open Excel, import that file, and create the chart."

You: *gets what you asked for*
```

The cool part? The system handles all the "passing information between agents" stuff automatically. You don't have to manually serialize data or manage context. It just... works.

---

## What Makes It Different

### It Actually Understands What You Want

Instead of hard-coding "click button at X,Y coordinates," it uses AI to understand what you're asking for and figures out the best way to do it. The manager agent breaks down your request into subtasks and assigns them to the right specialist.

### It Works Across Platforms

macOS? Windows? Linux? Doesn't matter. The system detects your platform and uses the right tools:

- **macOS**: Uses NSAccessibility (the same thing VoiceOver uses)
- **Windows**: Uses UI Automation (built into Windows)
- **Linux**: Uses AT-SPI (the accessibility standard)

All of these give you 100% accurate element detection when apps support accessibility. And when they don't? It falls back to reading the screen.

### It Has Fallbacks

Here's the accuracy system in plain English:

**Tier 1 (100% accurate)**: Uses your OS's accessibility features. Like asking the app directly "where's the submit button?" Works perfectly... when apps support it.

**Tier 2 (95-99% accurate)**: Reads the screen with OCR. It can find text even if the app doesn't expose it through accessibility. Uses EasyOCR, PaddleOCR, or macOS's built-in Vision framework depending on what's available.

**Tier 3 (85-95% accurate)**: Uses vision AI (like GPT-4V or Claude) to understand what's on screen. This is the fallback when text isn't readable or you need semantic understanding.

The system tries Tier 1 first, then falls back automatically. You don't have to think about it.

### It's Built for Real Work

- Phone verification? Handles it with Twilio integration
- Voice input? Talk to it instead of typing (Deepgram integration)
- File downloads? Tracks them automatically and passes paths to other agents
- Safety checks? Validates commands before running them
- Error handling? Actually tells you what went wrong

---

## Platform Support

### macOS (10.14+)

Works great on Mac. Uses atomacos to talk to NSAccessibility (the same API that powers VoiceOver). If you've ever used accessibility features on Mac, you know how reliable they are ‚Äî that's what this uses.

**What you need to do**: Grant accessibility permissions in System Settings. That's it. The installer will remind you.

**What works**: TextEdit, Calculator, Finder, System Settings, basically any app that supports accessibility (which is most of them).

### Windows (10+)

Uses pywinauto to talk to Windows UI Automation. Same thing Windows uses internally for accessibility features.

**What you need to do**: Run as Administrator for full access. That's really it.

**What works**: Calculator, Notepad, File Explorer, Settings app, most modern Windows apps.

### Linux (Ubuntu 20.04+, Debian 11+)

Uses pyatspi to talk to AT-SPI, which is the Linux accessibility standard. Works with GNOME, KDE, and other desktop environments.

**What you need to do**: Install `python3-pyatspi` (the installer handles this). Make sure you're running X11 (Wayland support is coming).

**What works**: gedit, GNOME Calculator, Files, most GTK apps.

---

## Installation

### The Easy Way

```bash
# Clone it
git clone https://github.com/yourusername/computer-use.git
cd computer-use

# Run the installer (it figures out your platform)
./install.sh
```

The installer does everything:

- Detects your OS
- Installs the right dependencies
- Sets up permissions (on Mac)
- Verifies everything works

Takes about 2 minutes. Seriously.

### The Manual Way (If You're Into That)

#### macOS

```bash
# Install UV (the package manager we use)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install with Mac-specific stuff
uv sync --extra macos

# Grant permissions
# System Settings ‚Üí Privacy & Security ‚Üí Accessibility
# Add Terminal (or your IDE) to the list
```

#### Windows

```bash
# Install UV
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Install with Windows stuff
uv sync --extra windows

# Run as Admin (recommended)
```

#### Linux

```bash
# Install UV
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install system deps
sudo apt-get update
sudo apt-get install -y python3-pyatspi python3-xlib

# Install with Linux stuff
uv sync --extra linux
```

---

## Configuration

You'll need API keys for the AI models. Don't worry, there are free options.

Create a `.env` file in the project root:

```bash
# Main LLM (for general tasks)
LLM_PROVIDER=google
LLM_MODEL=gemini-2.0-flash-exp  # Free tier available!

# Vision LLM (for reading screenshots)
VISION_LLM_PROVIDER=google
VISION_LLM_MODEL=gemini-2.0-flash-exp

# Browser LLM (for web automation)
BROWSER_LLM_PROVIDER=google
BROWSER_LLM_MODEL=gemini-2.0-flash-exp

# Your API key
GOOGLE_API_KEY=your_key_here
```

**Free option**: Google Gemini has a free tier. Perfect for getting started.

**If you want better accuracy**: Use OpenAI's GPT-4o or Anthropic's Claude. Costs money, but they're really good.

**For phone verification** (optional):

```bash
TWILIO_ACCOUNT_SID=your_sid
TWILIO_AUTH_TOKEN=your_token
TWILIO_PHONE_NUMBER=+1234567890
```

Only needed if you want automated phone verification for signups. The system works fine without it.

**For voice input** (optional):

```bash
DEEPGRAM_API_KEY=your_deepgram_key
VOICE_INPUT_LANGUAGE=multi  # or 'en', 'es', 'fr', etc. 'multi' supports 100+ languages
```

Want to talk to your computer instead of typing? Get a Deepgram API key (they have a free tier). Set `VOICE_INPUT_LANGUAGE=multi` for automatic language detection, or specify a language like `en` for English. The system will let you toggle between text and voice modes with F5.

---

## Usage

### Basic Usage

```bash
# Just run it
uv run python -m computer_use.main

# Or start with voice input mode enabled
uv run python -m computer_use.main --voice-input
```

Then type what you want (or speak, if you've got voice input set up):

```
üí¨ Enter your task:
‚û§ Download image of Cristiano Ronaldo
```

The system figures out:

- You want web automation (browser agent)
- It needs to search for images
- It needs to download one
- It handles all the details

### Voice Input

Tired of typing? You can talk to it instead. Here's how:

**Setup**:

1. Get a Deepgram API key from [deepgram.com](https://deepgram.com) (free tier available)
2. Add `DEEPGRAM_API_KEY=your_key` to your `.env` file
3. Optionally set `VOICE_INPUT_LANGUAGE=multi` for automatic language detection (supports 100+ languages)

**Using it**:

- Press **F5** to toggle between text and voice modes
- When in voice mode, speak your task and press Enter when done
- You'll see real-time transcription as you speak
- Works in multiple languages automatically (if you set `VOICE_INPUT_LANGUAGE=multi`)

**Example**:

```
üé§ Listening... (Press Enter to finish, Ctrl+C to cancel)
Using multilingual mode - supports 100+ languages automatically

‚û§ Download image of Cristiano Ronaldo
‚úÖ Transcribed: Download image of Cristiano Ronaldo
```

Pretty neat, right? No more typing long commands. Just say what you want and it figures it out.

### Real Examples

**Web stuff**:

```
> Download HD image of Cristiano Ronaldo
> Search for Tesla stock price and save to file
> Sign up for account on website with phone verification
```

**Desktop apps**:

```
> Open Calculator and compute 1234 √ó 5678
> Create new document in TextEdit with content "Hello World"
> Open System Settings and change theme to dark mode
```

**System stuff**:

```
> Create folder named "reports" in Documents
> Move all PDF files from Downloads to Documents
> List all Python files in current directory
```

**Complex workflows**:

```
> Download census data and create chart in Excel
> Research fashion trends on census.gov and create presentation
```

The system breaks these down automatically. "Download and create chart" becomes two tasks: browser downloads, GUI creates chart. The agents automatically pass the file between them.

### Programmatic Usage

If you want to use it in code:

```python
from computer_use.crew import ComputerUseCrew
from computer_use.utils.platform_detector import detect_platform
from computer_use.utils.safety_checker import SafetyChecker

# Set it up
capabilities = detect_platform()
safety_checker = SafetyChecker()
crew = ComputerUseCrew(capabilities, safety_checker)

# Use it
result = await crew.execute_task("Open Calculator and compute 25 √ó 36")

if result['overall_success']:
    print("‚úÖ Done!")
else:
    print(f"‚ùå Failed: {result.get('error')}")
```

---

## Troubleshooting

### macOS: "Accessibility permission denied"

This happens a lot. Here's how to fix it:

1. Open System Settings
2. Go to Privacy & Security ‚Üí Accessibility
3. Click the lock icon (bottom left)
4. Click the "+" button
5. Add Terminal (or your IDE)
6. Restart Terminal/IDE

The app will remind you if permissions aren't set up.

### Windows: "UI Automation not available"

Run PowerShell or Terminal as Administrator. Right-click ‚Üí "Run as Administrator". That's usually it.

### Linux: "AT-SPI not available"

```bash
sudo apt-get install -y python3-pyatspi python3-xlib
```

Make sure you're running X11, not Wayland (for now).

### OCR Not Working

Try switching engines. The system defaults to EasyOCR, but you can use:

- PaddleOCR (lighter, faster)
- macOS Vision (on Mac, really good)

Check the config files if you want to change defaults.

### "Invalid API key"

Check your `.env` file. Make sure:

- No quotes around the key value
- The key is actually valid (test it in the provider's console)
- You're using the right environment variable name

### Voice Input Not Working

**"DEEPGRAM_API_KEY not found"**:

- Add `DEEPGRAM_API_KEY=your_key` to your `.env` file
- Get a key from [deepgram.com](https://deepgram.com) (free tier works great)

**"No microphone detected"**:

- Make sure your microphone is connected and working
- Check system permissions (macOS might need microphone access)
- Try a different microphone if you have one

**Voice input dependencies missing**:

```bash
# Install voice input dependencies
pip install deepgram-sdk sounddevice
```

**Language detection not working**:

- Set `VOICE_INPUT_LANGUAGE=multi` in your `.env` for automatic detection
- Or specify a language: `VOICE_INPUT_LANGUAGE=en` for English, `es` for Spanish, etc.

### General Debugging

```bash
# Enable verbose logging
export LOG_LEVEL=DEBUG
uv run python -m computer_use.main
```

This shows you what's happening under the hood. Useful when something's not working and you want to see why.

---

## Performance

Here's what to expect:

- **Task analysis**: 1-2 seconds (depends on your LLM)
- **Accessibility detection**: Under 100ms (basically instant)
- **OCR detection**: 200-500ms (depends on your CPU)
- **Vision AI**: 1-3 seconds (depends on API latency)
- **Click execution**: Under 50ms

For a typical task like "download file and open in app," you're looking at 10-30 seconds total. Most of that is the AI thinking and the web automation. The actual clicking and typing? Nearly instant.

---

## How It Actually Works (Technical Details)

### The Agents

**Manager Agent**: The coordinator. Takes your request, figures out what needs to happen, breaks it into subtasks, assigns them to specialists. Uses an LLM to understand what you want.

**Browser Agent**: Web specialist. Handles navigation, downloads, forms, phone verification. Uses Browser-Use under the hood, which is really good at web automation.

**GUI Agent**: Desktop app specialist. Handles clicking, typing, reading screens. Uses the multi-tier accuracy system ‚Äî tries accessibility first, falls back to OCR, then vision AI.

**System Agent**: Command-line specialist. Runs shell commands safely. Validates everything before executing.

### The Accuracy System

**Tier 1**: Uses your OS's accessibility APIs. On Mac, that's NSAccessibility (via atomacos). On Windows, UI Automation (via pywinauto). On Linux, AT-SPI (via pyatspi). These give you 100% accurate coordinates because the OS tells you exactly where things are.

**Tier 2**: Reads the screen with OCR. EasyOCR supports 80+ languages. PaddleOCR is lighter and faster. macOS Vision is really good if you're on Mac. These find text even when accessibility doesn't work.

**Tier 3**: Uses vision AI (GPT-4V, Claude, Gemini) to understand what's on screen. This is the fallback when text isn't readable or you need semantic understanding.

The system tries Tier 1 first, automatically falls back if needed. You don't configure this ‚Äî it just works.

### Agent Context Passing

When one agent finishes, it automatically passes its output to the next agent. So if the browser agent downloads a file, the GUI agent automatically knows where it is. No manual serialization, no string concatenation. The system handles it seamlessly.

---

## Contributing

We'd love your help! Here's how to get started:

```bash
# Clone and set up
git clone https://github.com/yourusername/computer-use.git
cd computer-use
uv sync --dev --extra macos  # or windows/linux

# Run tests
uv run pytest

# Check code style
uv run ruff check .
uv run ruff format .
```

**Code standards**:

- Max 400 lines per file (we're serious about this)
- Docstrings only (no inline comments)
- Type hints everywhere
- Keep it simple and modular

---

## License

MIT License ‚Äî use it however you want. Just don't blame us if your automation does something weird. (We've all been there.)

---

## Acknowledgments

This builds on some amazing open-source projects:

- **CrewAI** ‚Äî The multi-agent framework that makes this possible
- **Browser-Use** ‚Äî Web automation that actually works
- **LangChain** ‚Äî LLM integration made easy
- **EasyOCR** ‚Äî OCR that supports 80+ languages
- **OpenCV** ‚Äî Computer vision that's been around forever and still works great
- **PyAutoGUI** ‚Äî Cross-platform input control
- **atomacos** (Mac), **pywinauto** (Windows), **pyatspi** (Linux) ‚Äî Platform accessibility APIs
- **Deepgram** ‚Äî Voice-to-text transcription with multilingual support
- **sounddevice** ‚Äî Cross-platform audio capture

Without these, this project wouldn't exist. So thanks to everyone who built and maintains them.

---

**Questions? Issues? Want to contribute?**

Check out the [GitHub repository](https://github.com/yourusername/computer-use) or open an issue. We're here to help.
